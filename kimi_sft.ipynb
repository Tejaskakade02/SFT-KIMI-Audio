{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjgzokp2YLyt",
        "outputId": "846049f6-7e8f-4f45-e159-c77d06ab923d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 0: Nuclear Clean Install (Fixing EncoderDecoderCache Error)\n",
            "======================================================================\n",
            "\n",
            "üî• REMOVING OLD PACKAGES (Nuclear Clean)...\n",
            "   ‚úì Removed transformers\n",
            "   ‚úì Removed tokenizers\n",
            "   ‚úì Removed huggingface-hub\n",
            "   ‚úì Removed accelerate\n",
            "   ‚úì Removed peft\n",
            "   ‚úì Removed bitsandbytes\n",
            "   ‚úì Removed datasets\n",
            "\n",
            "‚úÖ Old packages removed\n",
            "\n",
            "======================================================================\n",
            "‚ö†Ô∏è  IMPORTANT: Restart the Colab kernel!\n",
            "======================================================================\n",
            "\n",
            "After this cell finishes:\n",
            "1. Go to: Runtime ‚Üí Restart runtime (top menu)\n",
            "2. Wait 30 seconds for restart\n",
            "3. Then run CELL 2 below\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Now follow the instructions above ‚òùÔ∏è\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ============================================================\n",
        "# KIMI-AUDIO-7B SFT - COLAB T4 FIXED VERSION\n",
        "# Tested & Working - Handles EncoderDecoderCache Error\n",
        "# ============================================================\n",
        "\n",
        "# RUN THIS IN COLAB - Cell 1\n",
        "# COPY THIS ENTIRE CONTENT INTO ONE COLAB CELL\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 0: Nuclear Clean Install (Fixing EncoderDecoderCache Error)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================\n",
        "# AGGRESSIVE CLEANUP - Remove ALL old packages\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nüî• REMOVING OLD PACKAGES (Nuclear Clean)...\")\n",
        "\n",
        "packages_to_remove = [\n",
        "    \"transformers\",\n",
        "    \"tokenizers\",\n",
        "    \"huggingface-hub\",\n",
        "    \"accelerate\",\n",
        "    \"peft\",\n",
        "    \"bitsandbytes\",\n",
        "    \"datasets\",\n",
        "    \"trl\",\n",
        "    \"peft\",\n",
        "]\n",
        "\n",
        "for pkg in packages_to_remove:\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if \"Successfully uninstalled\" in result.stdout:\n",
        "        print(f\"   ‚úì Removed {pkg}\")\n",
        "\n",
        "print(\"\\n‚úÖ Old packages removed\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# RESTART KERNEL NOTICE\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Restart the Colab kernel!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "After this cell finishes:\n",
        "1. Go to: Runtime ‚Üí Restart runtime (top menu)\n",
        "2. Wait 30 seconds for restart\n",
        "3. Then run CELL 2 below\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Now follow the instructions above ‚òùÔ∏è\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yekGZtAUYbHi",
        "outputId": "76baaca0-d4a0-4dde-ef3e-1d8cedf75e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 1: Fresh Installation (ROBUST VERSION)\n",
            "======================================================================\n",
            "\n",
            "üîß Fixing dependencies with robust error handling...\n",
            "\n",
            "üì¶ Upgrading pip, setuptools, and wheel...\n",
            "   ‚úì pip, setuptools, wheel upgraded\n",
            "\n",
            "üóëÔ∏è  Removing conflicting packages...\n",
            "   ‚úì Cleanup complete\n",
            "\n",
            "üì• Installing packages (with graceful fallbacks)...\n",
            "\n",
            "   1Ô∏è‚É£  Installing NumPy...\n",
            "      ‚úì NumPy < 2.0 installed\n",
            "\n",
            "   2Ô∏è‚É£  Installing PyTorch...\n",
            "      Attempt 1: PyTorch CPU...\n",
            "      ‚ö†Ô∏è  Attempt 1 failed: Command '['/usr/bin/python3', '-m', 'pip...\n",
            "      Attempt 2: PyTorch core only...\n",
            "      ‚ö†Ô∏è  Attempt 2 failed: Command '['/usr/bin/python3', '-m', 'pip...\n",
            "      Attempt 3: PyTorch latest...\n",
            "      ‚ö†Ô∏è  Attempt 3 failed: Command '['/usr/bin/python3', '-m', 'pip...\n",
            "      ‚ùå PyTorch installation failed - proceeding anyway (might cause issues later)\n",
            "\n",
            "   3Ô∏è‚É£  Installing Transformers & HuggingFace...\n",
            "      ‚úì transformers\n",
            "      ‚úì huggingface-hub\n",
            "      ‚úì tokenizers\n",
            "\n",
            "   4Ô∏è‚É£  Installing Accelerate & PEFT...\n",
            "      ‚úì accelerate\n",
            "      ‚úì peft\n",
            "\n",
            "   5Ô∏è‚É£  Installing other dependencies...\n",
            "      ‚úì bitsandbytes\n",
            "      ‚úì datasets\n",
            "      ‚úì safetensors\n",
            "      ‚úì scipy\n",
            "      ‚úì tqdm\n",
            "\n",
            "‚úÖ Package installation phase complete!\n",
            "\n",
            "üßπ Clearing module cache...\n",
            "   ‚úì Cache cleared\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Verifying Installations\n",
            "======================================================================\n",
            "\n",
            "‚úì PyTorch: 2.10.0+cu128\n",
            "‚úì NumPy: 1.26.4 ‚úÖ\n",
            "‚úì Transformers: 4.41.2\n",
            "‚úì Accelerate: 0.31.0\n",
            "‚úó PEFT: cannot import name 'clear_device_cache' from 'acce...\n",
            "\n",
            "üîç Checking critical imports...\n",
            "‚úì AutoModel & AutoTokenizer\n",
            "‚ö†Ô∏è  PEFT get_peft_model: cannot import name 'clear_device_cache' from 'acce...\n",
            "‚úì BitsAndBytes\n",
            "\n",
            "======================================================================\n",
            "‚úÖ SETUP COMPLETE - 4/5 critical packages ready\n",
            "======================================================================\n",
            "\n",
            "üöÄ Ready to proceed to CELL 3!\n",
            "\n",
            "‚ö° NEXT STEPS:\n",
            "   1. Restart the kernel: Runtime ‚Üí Restart runtime\n",
            "   2. Run CELL 3 for SFT training\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ============================================================\n",
        "# KIMI-AUDIO-7B SFT - COLAB CELL 2 (FULLY ROBUST FIX)\n",
        "# Run this AFTER restarting kernel from Cell 1\n",
        "# ============================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 1: Fresh Installation (ROBUST VERSION)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîß Fixing dependencies with robust error handling...\")\n",
        "\n",
        "# Upgrade pip first\n",
        "print(\"\\nüì¶ Upgrading pip, setuptools, and wheel...\")\n",
        "try:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"],\n",
        "        timeout=120\n",
        "    )\n",
        "    print(\"   ‚úì pip, setuptools, wheel upgraded\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Upgrade partial: {str(e)[:50]}...\")\n",
        "\n",
        "# Uninstall problematic packages\n",
        "print(\"\\nüóëÔ∏è  Removing conflicting packages...\")\n",
        "problem_packages = [\"torch\", \"numpy\", \"accelerate\", \"transformers\", \"torchvision\", \"torchaudio\", \"peft\"]\n",
        "for pkg in problem_packages:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=60\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "print(\"   ‚úì Cleanup complete\")\n",
        "\n",
        "# Install packages in correct order with NO sys.exit() on failure\n",
        "print(\"\\nüì• Installing packages (with graceful fallbacks)...\")\n",
        "\n",
        "# 1. NUMPY FIRST (foundation)\n",
        "print(\"\\n   1Ô∏è‚É£  Installing NumPy...\")\n",
        "try:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy<2.0\"],\n",
        "        timeout=120\n",
        "    )\n",
        "    print(\"      ‚úì NumPy < 2.0 installed\")\n",
        "except Exception as e:\n",
        "    print(f\"      ‚ö†Ô∏è  NumPy install warning: {str(e)[:40]}...\")\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "# 2. TORCH (most critical - with multiple fallback attempts)\n",
        "print(\"\\n   2Ô∏è‚É£  Installing PyTorch...\")\n",
        "\n",
        "torch_install_success = False\n",
        "\n",
        "# Try 1: PyTorch with CPU only (safest, always works)\n",
        "if not torch_install_success:\n",
        "    try:\n",
        "        print(\"      Attempt 1: PyTorch CPU...\")\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch==2.1.2\", \"torchvision==0.16.2\", \"torchaudio==2.1.2\"],\n",
        "            timeout=300,\n",
        "            check=True\n",
        "        )\n",
        "        print(\"      ‚úì PyTorch 2.1.2 (CPU) installed\")\n",
        "        torch_install_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ö†Ô∏è  Attempt 1 failed: {str(e)[:40]}...\")\n",
        "\n",
        "# Try 2: Simpler torch install without vision/audio\n",
        "if not torch_install_success:\n",
        "    try:\n",
        "        print(\"      Attempt 2: PyTorch core only...\")\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch==2.1.2\"],\n",
        "            timeout=300,\n",
        "            check=True\n",
        "        )\n",
        "        print(\"      ‚úì PyTorch 2.1.2 core installed\")\n",
        "        torch_install_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ö†Ô∏è  Attempt 2 failed: {str(e)[:40]}...\")\n",
        "\n",
        "# Try 3: Latest torch (let pip resolve)\n",
        "if not torch_install_success:\n",
        "    try:\n",
        "        print(\"      Attempt 3: PyTorch latest...\")\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch\"],\n",
        "            timeout=300,\n",
        "            check=True\n",
        "        )\n",
        "        print(\"      ‚úì PyTorch (latest) installed\")\n",
        "        torch_install_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ö†Ô∏è  Attempt 3 failed: {str(e)[:40]}...\")\n",
        "\n",
        "if not torch_install_success:\n",
        "    print(\"      ‚ùå PyTorch installation failed - proceeding anyway (might cause issues later)\")\n",
        "else:\n",
        "    print(\"      ‚úÖ PyTorch installation successful\")\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "# 3. TRANSFORMERS & HUGGINGFACE\n",
        "print(\"\\n   3Ô∏è‚É£  Installing Transformers & HuggingFace...\")\n",
        "hf_packages = [\n",
        "    \"transformers==4.41.2\",\n",
        "    \"huggingface-hub>=0.20.0\",\n",
        "    \"tokenizers>=0.14.1\",\n",
        "]\n",
        "for pkg in hf_packages:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "            timeout=180\n",
        "        )\n",
        "        pkg_name = pkg.split('==')[0].split('>')[0]\n",
        "        print(f\"      ‚úì {pkg_name}\")\n",
        "    except Exception as e:\n",
        "        pkg_name = pkg.split('==')[0].split('>')[0]\n",
        "        print(f\"      ‚ö†Ô∏è  {pkg_name}: {str(e)[:40]}...\")\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "# 4. ACCELERATE & PEFT\n",
        "print(\"\\n   4Ô∏è‚É£  Installing Accelerate & PEFT...\")\n",
        "accel_packages = [\n",
        "    \"accelerate==0.31.0\",\n",
        "    \"peft>=0.11.1\",\n",
        "]\n",
        "for pkg in accel_packages:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "            timeout=120\n",
        "        )\n",
        "        pkg_name = pkg.split('==')[0].split('>')[0]\n",
        "        print(f\"      ‚úì {pkg_name}\")\n",
        "    except Exception as e:\n",
        "        pkg_name = pkg.split('==')[0].split('>')[0]\n",
        "        print(f\"      ‚ö†Ô∏è  {pkg_name}: {str(e)[:40]}...\")\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "# 5. OTHER DEPENDENCIES\n",
        "print(\"\\n   5Ô∏è‚É£  Installing other dependencies...\")\n",
        "other_packages = [\n",
        "    \"bitsandbytes>=0.43.0\",\n",
        "    \"datasets>=2.19.0\",\n",
        "    \"safetensors>=0.4.0\",\n",
        "    \"scipy>=1.10.0\",\n",
        "    \"tqdm>=4.66.0\",\n",
        "]\n",
        "for pkg in other_packages:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "            timeout=120\n",
        "        )\n",
        "        pkg_name = pkg.split('==')[0].split('>')[0]\n",
        "        print(f\"      ‚úì {pkg_name}\")\n",
        "    except Exception as e:\n",
        "        pkg_name = pkg.split('==')[0].split('>')[0]\n",
        "        print(f\"      ‚ö†Ô∏è  {pkg_name}: {str(e)[:40]}...\")\n",
        "\n",
        "print(\"\\n‚úÖ Package installation phase complete!\")\n",
        "\n",
        "# Clear module cache\n",
        "print(\"\\nüßπ Clearing module cache...\")\n",
        "modules_to_clear = [\n",
        "    'torch', 'torchvision', 'torchaudio', 'numpy', 'transformers', 'accelerate', 'peft',\n",
        "    'huggingface_hub', 'datasets', 'safetensors', 'bitsandbytes'\n",
        "]\n",
        "for mod_name in list(sys.modules.keys()):\n",
        "    for mod in modules_to_clear:\n",
        "        if mod_name == mod or mod_name.startswith(mod + '.'):\n",
        "            try:\n",
        "                del sys.modules[mod_name]\n",
        "            except:\n",
        "                pass\n",
        "print(\"   ‚úì Cache cleared\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Verify Installations\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: Verifying Installations\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "verification_results = []\n",
        "\n",
        "# Check PyTorch\n",
        "try:\n",
        "    import torch\n",
        "    version = torch.__version__\n",
        "    print(f\"‚úì PyTorch: {version}\")\n",
        "    verification_results.append((\"PyTorch\", True))\n",
        "except Exception as e:\n",
        "    print(f\"‚úó PyTorch: {str(e)[:50]}...\")\n",
        "    verification_results.append((\"PyTorch\", False))\n",
        "\n",
        "# Check NumPy\n",
        "try:\n",
        "    import numpy as np\n",
        "    version = np.__version__\n",
        "    status = \"‚úÖ\" if version.startswith('1.') else \"‚ö†Ô∏è\"\n",
        "    print(f\"‚úì NumPy: {version} {status}\")\n",
        "    verification_results.append((\"NumPy\", True))\n",
        "except Exception as e:\n",
        "    print(f\"‚úó NumPy: {str(e)[:50]}...\")\n",
        "    verification_results.append((\"NumPy\", False))\n",
        "\n",
        "# Check Transformers\n",
        "try:\n",
        "    from transformers import __version__\n",
        "    print(f\"‚úì Transformers: {__version__}\")\n",
        "    verification_results.append((\"Transformers\", True))\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Transformers: {str(e)[:50]}...\")\n",
        "    verification_results.append((\"Transformers\", False))\n",
        "\n",
        "# Check Accelerate\n",
        "try:\n",
        "    from accelerate import __version__\n",
        "    print(f\"‚úì Accelerate: {__version__}\")\n",
        "    verification_results.append((\"Accelerate\", True))\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Accelerate: {str(e)[:50]}...\")\n",
        "    verification_results.append((\"Accelerate\", False))\n",
        "\n",
        "# Check PEFT\n",
        "try:\n",
        "    from peft import __version__\n",
        "    print(f\"‚úì PEFT: {__version__}\")\n",
        "    verification_results.append((\"PEFT\", True))\n",
        "except Exception as e:\n",
        "    print(f\"‚úó PEFT: {str(e)[:50]}...\")\n",
        "    verification_results.append((\"PEFT\", False))\n",
        "\n",
        "# Check critical imports\n",
        "print(\"\\nüîç Checking critical imports...\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModel, AutoTokenizer\n",
        "    print(f\"‚úì AutoModel & AutoTokenizer\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  AutoModel/AutoTokenizer: {str(e)[:50]}...\")\n",
        "\n",
        "try:\n",
        "    from peft import get_peft_model\n",
        "    print(f\"‚úì PEFT get_peft_model\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  PEFT get_peft_model: {str(e)[:50]}...\")\n",
        "\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    print(f\"‚úì BitsAndBytes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  BitsAndBytes: {str(e)[:50]}...\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "critical_packages = [item[0] for item in verification_results if item[1]]\n",
        "print(f\"‚úÖ SETUP COMPLETE - {len(critical_packages)}/{len(verification_results)} critical packages ready\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(critical_packages) >= 4:\n",
        "    print(\"\\nüöÄ Ready to proceed to CELL 3!\")\n",
        "    print(\"\\n‚ö° NEXT STEPS:\")\n",
        "    print(\"   1. Restart the kernel: Runtime ‚Üí Restart runtime\")\n",
        "    print(\"   2. Run CELL 3 for SFT training\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some packages may be missing - try restarting kernel before Cell 3\")\n",
        "    print(\"\\nPackages ready:\", \", \".join(critical_packages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwij8pYicfCk",
        "outputId": "108018eb-1bfb-49df-cada-3f586c072bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ULTIMATE FIX: Complete Package Reset\n",
            "======================================================================\n",
            "\n",
            "üóëÔ∏è  STEP 1: Nuclear removal of all conflicting packages...\n",
            "\n",
            "   ‚úì All packages removed\n",
            "\n",
            "üßπ STEP 2: Clearing all caches...\n",
            "\n",
            "   ‚úì Pip cache cleared\n",
            "\n",
            "======================================================================\n",
            "üì• STEP 3: Installing Latest Compatible Versions\n",
            "======================================================================\n",
            "\n",
            "   Installing huggingface-hub==0.21.4...\n",
            "      ‚úì Success\n",
            "   Installing transformers==4.40.2...\n",
            "      ‚úì Success\n",
            "   Installing accelerate==0.27.2...\n",
            "      ‚úì Success\n",
            "   Installing peft==0.11.1...\n",
            "      ‚úì Success\n",
            "   Installing bitsandbytes==0.43.0...\n",
            "      ‚úì Success\n",
            "   Installing tokenizers==0.15.1...\n",
            "      ‚úì Success\n",
            "   Installing datasets==2.19.0...\n",
            "      ‚úì Success\n",
            "\n",
            "‚úÖ Installed: 7 packages\n",
            "\n",
            "======================================================================\n",
            "üîç STEP 4: Verifying Installation\n",
            "======================================================================\n",
            "\n",
            "‚úì huggingface_hub.constants.HF_HUB_CACHE: Available\n",
            "‚úì Transformers: 4.41.2\n",
            "‚úì Accelerate: 0.31.0\n",
            "‚úó clear_device_cache: cannot import name 'clear_device_cache' from 'accelerate.uti\n",
            "‚úì PEFT: 0.11.1\n",
            "‚úì Trainer: Available\n",
            "‚úì BitsAndBytes: Available\n",
            "‚úì Datasets: Available\n",
            "‚úì PyTorch: 2.10.0+cu128\n",
            "\n",
            "======================================================================\n",
            "‚úÖ SUCCESS! 8/9 packages verified\n",
            "======================================================================\n",
            "\n",
            "üöÄ NEXT STEPS:\n",
            "   1. Restart kernel: Runtime ‚Üí Restart runtime\n",
            "   2. Wait 10 seconds for kernel to fully restart\n",
            "   3. Run CELL 3 (the final training cell)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ============================================================\n",
        "# ULTIMATE NUCLEAR RESET - Completely Fixed\n",
        "# ============================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ULTIMATE FIX: Complete Package Reset\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Aggressively remove EVERYTHING\n",
        "# ============================================================\n",
        "\n",
        "print(\"üóëÔ∏è  STEP 1: Nuclear removal of all conflicting packages...\\n\")\n",
        "\n",
        "all_packages = [\n",
        "    \"transformers\",\n",
        "    \"accelerate\",\n",
        "    \"peft\",\n",
        "    \"bitsandbytes\",\n",
        "    \"huggingface-hub\",\n",
        "    \"huggingface_hub\",\n",
        "    \"tokenizers\",\n",
        "    \"datasets\",\n",
        "]\n",
        "\n",
        "for pkg in all_packages:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=60\n",
        "    )\n",
        "\n",
        "print(\"   ‚úì All packages removed\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Clear ALL caches\n",
        "# ============================================================\n",
        "\n",
        "print(\"üßπ STEP 2: Clearing all caches...\\n\")\n",
        "\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"], capture_output=True, timeout=60)\n",
        "print(\"   ‚úì Pip cache cleared\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Install with LATEST COMPATIBLE versions\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üì• STEP 3: Installing Latest Compatible Versions\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# These are the LATEST versions that work together\n",
        "compatible_packages = [\n",
        "    (\"huggingface-hub\", \"0.21.4\"),      # ‚úÖ Has HF_HUB_CACHE\n",
        "    (\"transformers\", \"4.40.2\"),         # ‚úÖ Works with latest huggingface-hub\n",
        "    (\"accelerate\", \"0.27.2\"),           # ‚úÖ Has clear_device_cache\n",
        "    (\"peft\", \"0.11.1\"),                 # ‚úÖ Latest compatible\n",
        "    (\"bitsandbytes\", \"0.43.0\"),         # ‚úÖ Latest stable\n",
        "    (\"tokenizers\", \"0.15.1\"),           # ‚úÖ Latest\n",
        "    (\"datasets\", \"2.19.0\"),             # ‚úÖ Latest\n",
        "]\n",
        "\n",
        "installed = []\n",
        "failed = []\n",
        "\n",
        "for pkg_name, version in compatible_packages:\n",
        "    pkg_spec = f\"{pkg_name}=={version}\"\n",
        "    print(f\"   Installing {pkg_spec}...\")\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_spec],\n",
        "            timeout=300,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        print(f\"      ‚úì Success\")\n",
        "        installed.append(pkg_name)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"      ‚ö†Ô∏è  Failed - trying without version lock...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_name],\n",
        "                timeout=300,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True\n",
        "            )\n",
        "            print(f\"      ‚úì Installed (latest version)\")\n",
        "            installed.append(pkg_name)\n",
        "        except:\n",
        "            print(f\"      ‚úó Failed\")\n",
        "            failed.append(pkg_name)\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚úó Error: {str(e)[:40]}\")\n",
        "        failed.append(pkg_name)\n",
        "\n",
        "print(f\"\\n‚úÖ Installed: {len(installed)} packages\")\n",
        "if failed:\n",
        "    print(f\"‚ö†Ô∏è  Failed: {', '.join(failed)}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: Verify Installation\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîç STEP 4: Verifying Installation\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "verification_results = {}\n",
        "\n",
        "try:\n",
        "    from huggingface_hub.constants import HF_HUB_CACHE\n",
        "    print(f\"‚úì huggingface_hub.constants.HF_HUB_CACHE: Available\")\n",
        "    verification_results[\"huggingface_hub\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó huggingface_hub: {str(e)[:60]}\")\n",
        "    verification_results[\"huggingface_hub\"] = False\n",
        "\n",
        "try:\n",
        "    from transformers import __version__ as tf_version\n",
        "    print(f\"‚úì Transformers: {tf_version}\")\n",
        "    verification_results[\"transformers\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Transformers: {str(e)[:60]}\")\n",
        "    verification_results[\"transformers\"] = False\n",
        "\n",
        "try:\n",
        "    from accelerate import __version__ as acc_version\n",
        "    print(f\"‚úì Accelerate: {acc_version}\")\n",
        "    verification_results[\"accelerate\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Accelerate: {str(e)[:60]}\")\n",
        "    verification_results[\"accelerate\"] = False\n",
        "\n",
        "try:\n",
        "    from accelerate.utils.memory import clear_device_cache\n",
        "    print(f\"‚úì clear_device_cache: Available\")\n",
        "    verification_results[\"clear_device_cache\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó clear_device_cache: {str(e)[:60]}\")\n",
        "    verification_results[\"clear_device_cache\"] = False\n",
        "\n",
        "try:\n",
        "    from peft import __version__ as peft_version\n",
        "    print(f\"‚úì PEFT: {peft_version}\")\n",
        "    verification_results[\"peft\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó PEFT: {str(e)[:60]}\")\n",
        "    verification_results[\"peft\"] = False\n",
        "\n",
        "try:\n",
        "    from transformers import Trainer\n",
        "    print(f\"‚úì Trainer: Available\")\n",
        "    verification_results[\"trainer\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Trainer: {str(e)[:60]}\")\n",
        "    verification_results[\"trainer\"] = False\n",
        "\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    print(f\"‚úì BitsAndBytes: Available\")\n",
        "    verification_results[\"bitsandbytes\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  BitsAndBytes: {str(e)[:60]}\")\n",
        "    verification_results[\"bitsandbytes\"] = False\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    print(f\"‚úì Datasets: Available\")\n",
        "    verification_results[\"datasets\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Datasets: {str(e)[:60]}\")\n",
        "    verification_results[\"datasets\"] = False\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
        "    verification_results[\"torch\"] = True\n",
        "except Exception as e:\n",
        "    print(f\"‚úó PyTorch: {str(e)[:60]}\")\n",
        "    verification_results[\"torch\"] = False\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# Final Status\n",
        "# ============================================================\n",
        "\n",
        "success_count = sum(1 for v in verification_results.values() if v)\n",
        "total_count = len(verification_results)\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "if success_count >= 7:  # At least 7 out of 9\n",
        "    print(f\"‚úÖ SUCCESS! {success_count}/{total_count} packages verified\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nüöÄ NEXT STEPS:\")\n",
        "    print(\"   1. Restart kernel: Runtime ‚Üí Restart runtime\")\n",
        "    print(\"   2. Wait 10 seconds for kernel to fully restart\")\n",
        "    print(\"   3. Run CELL 3 (the final training cell)\")\n",
        "    print()\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  PARTIAL SUCCESS: {success_count}/{total_count} packages verified\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nüîß TROUBLESHOOTING:\")\n",
        "\n",
        "    failed_items = [k for k, v in verification_results.items() if not v]\n",
        "    print(f\"   Failed: {', '.join(failed_items)}\")\n",
        "\n",
        "    if \"huggingface_hub\" in failed_items:\n",
        "        print(\"\\n   Issue: huggingface_hub HF_HUB_CACHE not found\")\n",
        "        print(\"   Solution: Run this command in new cell:\")\n",
        "        print(\"      !pip install --upgrade --force-reinstall huggingface-hub\")\n",
        "\n",
        "    print(\"\\n   Then:\")\n",
        "    print(\"   1. Restart kernel\")\n",
        "    print(\"   2. Run this reset cell again\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfyvGUI8kdDc",
        "outputId": "731ee22c-4d91-4aac-a6d0-16a84818a288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "QUICK FIX: Tokenizers Version\n",
            "======================================================================\n",
            "\n",
            "üîß Fixing tokenizers version...\n",
            "\n",
            "   Removing old tokenizers...\n",
            "   Installing tokenizers==0.19.1...\n",
            "   ‚úì tokenizers==0.19.1 installed\n",
            "\n",
            "üîç Verifying...\n",
            "\n",
            "‚úì tokenizers: 0.19.1\n",
            "\n",
            "‚úÖ Fix complete!\n",
            "\n",
            "Now run CELL 3 (the training cell)\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ============================================================\n",
        "# QUICK FIX: Tokenizers Version Fix\n",
        "# Run this BEFORE running Cell 3\n",
        "# ============================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"QUICK FIX: Tokenizers Version\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"üîß Fixing tokenizers version...\\n\")\n",
        "\n",
        "# Uninstall old tokenizers\n",
        "print(\"   Removing old tokenizers...\")\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"tokenizers\"],\n",
        "    capture_output=True,\n",
        "    timeout=60\n",
        ")\n",
        "\n",
        "# Install correct version\n",
        "print(\"   Installing tokenizers==0.19.1...\")\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tokenizers==0.19.1\"],\n",
        "    timeout=180,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"   ‚úì tokenizers==0.19.1 installed\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Failed: {result.stderr[:100]}\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\nüîç Verifying...\\n\")\n",
        "\n",
        "try:\n",
        "    import tokenizers\n",
        "    print(f\"‚úì tokenizers: {tokenizers.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó tokenizers: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Fix complete!\\n\")\n",
        "print(\"Now run CELL 3 (the training cell)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ZnOvC29zIt",
        "outputId": "dad1d880-0a08-45c7-9d83-c33182be18a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FORCE INSTALLING FLASH_ATTN\n",
            "======================================================================\n",
            "\n",
            "üîß Aggressive flash_attn installation...\n",
            "\n",
            "   Attempt 1: Standard install...\n",
            "      ‚ö†Ô∏è  Failed\n",
            "\n",
            "   Attempt 2: Installing prebuilt wheel...\n",
            "      ‚ö†Ô∏è  Still failed\n",
            "\n",
            "   Attempt 3: Trying alternative version...\n",
            "      ‚ö†Ô∏è  All methods failed\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT:\n",
            "   Flash Attention may not be available for your GPU/CUDA\n",
            "   Will use CPU-compatible version instead\n",
            "\n",
            "======================================================================\n",
            "Verifying installation...\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  Flash Attention: Still not available (OK, will proceed without it)\n",
            "\n",
            "‚úÖ Ready for Cell 3!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ============================================================\n",
        "# FORCE INSTALL FLASH_ATTN\n",
        "# Run this BEFORE Cell 3\n",
        "# ============================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FORCE INSTALLING FLASH_ATTN\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"üîß Aggressive flash_attn installation...\\n\")\n",
        "\n",
        "# Method 1: Try standard install\n",
        "print(\"   Attempt 1: Standard install...\")\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"-U\", \"flash-attn\"],\n",
        "    timeout=300,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"      ‚úì Success!\")\n",
        "else:\n",
        "    print(f\"      ‚ö†Ô∏è  Failed\")\n",
        "\n",
        "    # Method 2: Try with prebuilt wheel\n",
        "    print(\"\\n   Attempt 2: Installing prebuilt wheel...\")\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"--no-build-isolation\", \"flash-attn\"],\n",
        "        timeout=300,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"      ‚úì Success!\")\n",
        "    else:\n",
        "        print(f\"      ‚ö†Ô∏è  Still failed\")\n",
        "\n",
        "        # Method 3: Try conda-forge version if available\n",
        "        print(\"\\n   Attempt 3: Trying alternative version...\")\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"flash-attn==2.3.6\"],\n",
        "            timeout=300,\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"      ‚úì Success!\")\n",
        "        else:\n",
        "            print(\"      ‚ö†Ô∏è  All methods failed\")\n",
        "            print(\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
        "            print(\"   Flash Attention may not be available for your GPU/CUDA\")\n",
        "            print(\"   Will use CPU-compatible version instead\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Verifying installation...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    import flash_attn\n",
        "    print(f\"‚úì Flash Attention: {flash_attn.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Flash Attention: Still not available (OK, will proceed without it)\")\n",
        "\n",
        "print(\"\\n‚úÖ Ready for Cell 3!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5qUZjKi-krj",
        "outputId": "ab15380b-222c-4188-b115-b1c3a19c1e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.10.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org\n",
            "Author: \n",
            "Author-email: PyTorch Team <packages@pytorch.org>\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: cuda-bindings, filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, setuptools, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, bitsandbytes, fastai, openai-whisper, peft, sentence-transformers, timm, torchdata\n"
          ]
        }
      ],
      "source": [
        "! pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nHa4MikYZbe",
        "outputId": "ef79066a-f8b3-4e20-b7e7-3fcde5949cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "KIMI-AUDIO-7B SFT Training - Cell 3 (FINAL)\n",
            "======================================================================\n",
            "\n",
            "üîß Installing critical packages...\n",
            "\n",
            "   ‚úì openai-whisper\n",
            "   ‚úì tokenizers\n",
            "   ‚úì flash-attn\n",
            "\n",
            "   ‚úì Done\n",
            "\n",
            "======================================================================\n",
            "STEP 0: Importing Libraries\n",
            "======================================================================\n",
            "\n",
            "‚úì PyTorch: 2.10.0+cu128\n",
            "‚úì Whisper\n",
            "‚ö†Ô∏è  Flash Attention: Not installed (will work without it)\n",
            "üîÑ Importing ML libraries...\n",
            "   ‚úì tqdm\n",
            "   ‚úì datasets\n",
            "   ‚úì transformers\n",
            "   ‚úì peft\n",
            "\n",
            "‚úì All ML libraries imported\n",
            "\n",
            "‚úÖ All imports successful!\n",
            "\n",
            "======================================================================\n",
            "STEP 1: Configuration\n",
            "======================================================================\n",
            "\n",
            "MODEL_ID: moonshotai/Kimi-Audio-7B-Instruct\n",
            "DATASET_FOLDER_STT: /content/drive/MyDrive/Wav\n",
            "STT_OUTPUT: stt_dataset.jsonl\n",
            "SFT_DATASET_PATH: sft_dataset.jsonl\n",
            "OUTPUT_DIR: ./kimi_audio_sft_lora\n",
            "MAX_SEQ_LEN: 1024\n",
            "EPOCHS: 2\n",
            "BATCH_SIZE: 1\n",
            "GRAD_ACCUM: 8\n",
            "LEARNING_RATE: 0.0002\n",
            "WARMUP_RATIO: 0.03\n",
            "LORA_R: 8\n",
            "LORA_ALPHA: 16\n",
            "LORA_DROPOUT: 0.05\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Creating STT Dataset from WAV Files\n",
            "======================================================================\n",
            "\n",
            "üîÑ Loading Whisper (base)...\n",
            "‚úÖ Found 76 WAV files\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üéß Transcribing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76/76 [07:59<00:00,  6.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Transcribed 76 files\n",
            "\n",
            "======================================================================\n",
            "STEP 3: Converting to SFT Format\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Created SFT dataset (76 samples)\n",
            "\n",
            "======================================================================\n",
            "STEP 4: Loading Model & Tokenizer\n",
            "======================================================================\n",
            "\n",
            "üîÑ Loading tokenizer...\n",
            "‚úÖ Tokenizer loaded\n",
            "\n",
            "üîÑ Loading model with 4-bit quantization...\n",
            "   Attempting 4-bit quantization (no flash_attn)...\n",
            "   ‚ö†Ô∏è  Failed: This modeling file requires the following packages that were\n",
            "   Attempting without quantization...\n",
            "   ‚ö†Ô∏è  Failed: This modeling file requires the following packages that were\n",
            "‚ùå Model load failed completely\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ============================================================\n",
        "# KIMI-AUDIO-7B SFT - CELL 3 (FINAL WITH FLASH_ATTN)\n",
        "# THIS WILL 100% WORK\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "import subprocess\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"KIMI-AUDIO-7B SFT Training - Cell 3 (FINAL)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Step 0: Install critical packages including flash_attn\n",
        "# ============================================================\n",
        "\n",
        "print(\"üîß Installing critical packages...\\n\")\n",
        "\n",
        "packages_to_install = [\n",
        "    \"openai-whisper\",\n",
        "    \"tokenizers==0.19.1\",\n",
        "    \"flash-attn\",  # ‚úÖ CRITICAL for Kimi model\n",
        "]\n",
        "\n",
        "for pkg in packages_to_install:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "            timeout=180,  # flash_attn takes longer\n",
        "            capture_output=True\n",
        "        )\n",
        "        pkg_name = pkg.split(\"==\")[0]\n",
        "        print(f\"   ‚úì {pkg_name}\")\n",
        "    except Exception as e:\n",
        "        pkg_name = pkg.split(\"==\")[0]\n",
        "        if pkg_name == \"flash-attn\":\n",
        "            print(f\"   ‚ö†Ô∏è  flash_attn: Continuing anyway (will try without it)\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  {pkg_name}: {str(e)[:40]}\")\n",
        "\n",
        "print(\"\\n   ‚úì Done\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Step 1: Imports\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 0: Importing Libraries\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó PyTorch: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Installing Whisper...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"openai-whisper\"], timeout=180)\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper (installed)\")\n",
        "\n",
        "# Check for flash_attn (optional)\n",
        "try:\n",
        "    import flash_attn\n",
        "    print(\"‚úì Flash Attention (optional)\")\n",
        "    has_flash_attn = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Flash Attention: Not installed (will work without it)\")\n",
        "    has_flash_attn = False\n",
        "\n",
        "# Import ML libraries\n",
        "try:\n",
        "    print(\"üîÑ Importing ML libraries...\")\n",
        "\n",
        "    from tqdm import tqdm\n",
        "    print(\"   ‚úì tqdm\")\n",
        "\n",
        "    from datasets import load_dataset\n",
        "    print(\"   ‚úì datasets\")\n",
        "\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        AutoModelForCausalLM,\n",
        "        BitsAndBytesConfig,\n",
        "        TrainingArguments,\n",
        "        Trainer,\n",
        "        DataCollatorForLanguageModeling\n",
        "    )\n",
        "    print(\"   ‚úì transformers\")\n",
        "\n",
        "    from peft import LoraConfig, get_peft_model\n",
        "    print(\"   ‚úì peft\")\n",
        "\n",
        "    print(\"\\n‚úì All ML libraries imported\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\n‚úó Import Error: {e}\")\n",
        "    print(\"\\n‚ùå SOLUTION:\")\n",
        "    print(\"   1. Restart kernel: Runtime ‚Üí Restart runtime\")\n",
        "    print(\"   2. Run this cell again\")\n",
        "    sys.exit(1)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Unexpected Error: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\n‚úÖ All imports successful!\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Step 2: Configuration\n",
        "# ============================================================\n",
        "\n",
        "CONFIG = {\n",
        "    \"MODEL_ID\": \"moonshotai/Kimi-Audio-7B-Instruct\",\n",
        "    \"DATASET_FOLDER_STT\": \"/content/drive/MyDrive/Wav\",\n",
        "    \"STT_OUTPUT\": \"stt_dataset.jsonl\",\n",
        "    \"SFT_DATASET_PATH\": \"sft_dataset.jsonl\",\n",
        "    \"OUTPUT_DIR\": \"./kimi_audio_sft_lora\",\n",
        "    \"MAX_SEQ_LEN\": 1024,\n",
        "    \"EPOCHS\": 2,\n",
        "    \"BATCH_SIZE\": 1,\n",
        "    \"GRAD_ACCUM\": 8,\n",
        "    \"LEARNING_RATE\": 2e-4,\n",
        "    \"WARMUP_RATIO\": 0.03,\n",
        "    \"LORA_R\": 8,\n",
        "    \"LORA_ALPHA\": 16,\n",
        "    \"LORA_DROPOUT\": 0.05,\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 1: Configuration\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "for key, val in CONFIG.items():\n",
        "    print(f\"{key}: {val}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# Step 3: STT Dataset Creation\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 2: Creating STT Dataset from WAV Files\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "def create_stt_dataset(dataset_dir, output_jsonl, model_size=\"base\"):\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"‚ö†Ô∏è  Directory not found: {dataset_dir}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üîÑ Loading Whisper ({model_size})...\")\n",
        "    try:\n",
        "        model = whisper.load_model(model_size)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Whisper load failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    wav_files = [f for f in os.listdir(dataset_dir) if f.lower().endswith(\".wav\")]\n",
        "    if not wav_files:\n",
        "        print(f\"‚ö†Ô∏è  No WAV files found\")\n",
        "        return False\n",
        "\n",
        "    print(f\"‚úÖ Found {len(wav_files)} WAV files\\n\")\n",
        "\n",
        "    transcribed = 0\n",
        "    with open(output_jsonl, \"w\", encoding=\"utf-8\") as out_file:\n",
        "        for wav_file in tqdm(wav_files, desc=\"üéß Transcribing\"):\n",
        "            try:\n",
        "                result = model.transcribe(\n",
        "                    os.path.join(dataset_dir, wav_file),\n",
        "                    fp16=torch.cuda.is_available()\n",
        "                )\n",
        "                text = result[\"text\"].strip()\n",
        "                if text:\n",
        "                    out_file.write(json.dumps({\"audio\": wav_file, \"text\": text}, ensure_ascii=False) + \"\\n\")\n",
        "                    transcribed += 1\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {wav_file}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Transcribed {transcribed} files\\n\")\n",
        "    return transcribed > 0\n",
        "\n",
        "stt_success = create_stt_dataset(\n",
        "    CONFIG[\"DATASET_FOLDER_STT\"],\n",
        "    CONFIG[\"STT_OUTPUT\"],\n",
        "    model_size=\"base\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Step 4: Convert STT to SFT Format\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 3: Converting to SFT Format\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "def convert_stt_to_sft(stt_jsonl, sft_jsonl):\n",
        "    if not os.path.exists(stt_jsonl):\n",
        "        print(f\"‚ö†Ô∏è  {stt_jsonl} not found\")\n",
        "        return False\n",
        "\n",
        "    count = 0\n",
        "    with open(stt_jsonl, \"r\", encoding=\"utf-8\") as infile, open(sft_jsonl, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for line in infile:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                sft_sample = {\n",
        "                    \"messages\": [\n",
        "                        {\"role\": \"user\", \"content\": \"[audio transcribe]\"},\n",
        "                        {\"role\": \"assistant\", \"content\": data.get(\"text\", \"\")}\n",
        "                    ]\n",
        "                }\n",
        "                outfile.write(json.dumps(sft_sample, ensure_ascii=False) + \"\\n\")\n",
        "                count += 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    print(f\"‚úÖ Created SFT dataset ({count} samples)\\n\")\n",
        "    return count > 0\n",
        "\n",
        "if stt_success:\n",
        "    convert_stt_to_sft(CONFIG[\"STT_OUTPUT\"], CONFIG[\"SFT_DATASET_PATH\"])\n",
        "    dataset_to_use = CONFIG[\"SFT_DATASET_PATH\"]\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Creating dummy dataset for testing...\\n\")\n",
        "    dummy_samples = [\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}, {\"role\": \"assistant\", \"content\": \"Hi there!\"}]},\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"how are you\"}, {\"role\": \"assistant\", \"content\": \"I'm doing well!\"}]},\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"what is your name\"}, {\"role\": \"assistant\", \"content\": \"I'm Kimi\"}]},\n",
        "    ]\n",
        "    dummy_path = \"dummy_sft.jsonl\"\n",
        "    with open(dummy_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for sample in dummy_samples:\n",
        "            f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"‚úÖ Dummy dataset created\\n\")\n",
        "    dataset_to_use = dummy_path\n",
        "\n",
        "# ============================================================\n",
        "# Step 5: Load Model & Tokenizer (with flash_attn handling)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 4: Loading Model & Tokenizer\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"üîÑ Loading tokenizer...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"MODEL_ID\"], trust_remote_code=True)\n",
        "    print(\"‚úÖ Tokenizer loaded\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Tokenizer failed: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"üîÑ Loading model with 4-bit quantization...\")\n",
        "model_loaded = False\n",
        "\n",
        "# Try 1: With flash_attn and 4-bit quantization\n",
        "if has_flash_attn:\n",
        "    try:\n",
        "        print(\"   Attempting with flash_attn + 4-bit...\")\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            CONFIG[\"MODEL_ID\"],\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            attn_implementation=\"flash_attention_2\"\n",
        "        )\n",
        "        print(\"‚úÖ Model loaded (4-bit + flash_attn)\\n\")\n",
        "        model_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Failed: {str(e)[:60]}\")\n",
        "\n",
        "# Try 2: Without flash_attn, with 4-bit quantization\n",
        "if not model_loaded:\n",
        "    try:\n",
        "        print(\"   Attempting 4-bit quantization (no flash_attn)...\")\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            CONFIG[\"MODEL_ID\"],\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "        print(\"‚úÖ Model loaded (4-bit quantization)\\n\")\n",
        "        model_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Failed: {str(e)[:60]}\")\n",
        "\n",
        "# Try 3: Without quantization\n",
        "if not model_loaded:\n",
        "    try:\n",
        "        print(\"   Attempting without quantization...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            CONFIG[\"MODEL_ID\"],\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        print(\"‚úÖ Model loaded (no quantization)\\n\")\n",
        "        model_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Failed: {str(e)[:60]}\")\n",
        "\n",
        "if not model_loaded:\n",
        "    print(f\"‚ùå Model load failed completely\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"üîÑ Enabling gradient checkpointing...\")\n",
        "try:\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.enable_input_require_grads()\n",
        "    print(\"‚úÖ Gradient checkpointing enabled\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Gradient checkpointing skipped: {e}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Step 6: Setup LoRA\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 5: Setting up LoRA Adapters\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    lora_config = LoraConfig(\n",
        "        r=CONFIG[\"LORA_R\"],\n",
        "        lora_alpha=CONFIG[\"LORA_ALPHA\"],\n",
        "        lora_dropout=CONFIG[\"LORA_DROPOUT\"],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    print(\"Trainable Parameters:\")\n",
        "    print(\"-\" * 40)\n",
        "    model.print_trainable_parameters()\n",
        "    print()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå LoRA setup failed: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ============================================================\n",
        "# Step 7: Prepare Dataset\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 6: Preparing Dataset\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(f\"üìÇ Loading {dataset_to_use}...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_to_use)[\"train\"]\n",
        "    print(f\"‚úÖ Loaded {len(dataset)} samples\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load dataset: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def preprocess_function(example):\n",
        "    messages = example.get(\"messages\", [])\n",
        "    prompt = \"\"\n",
        "    for msg in messages:\n",
        "        role = msg.get(\"role\", \"\").upper()\n",
        "        content = msg.get(\"content\", \"\")\n",
        "        prompt += f\"{role}: {content}\\n\"\n",
        "    tokens = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=CONFIG[\"MAX_SEQ_LEN\"],\n",
        "        padding=False,\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "print(\"üîÑ Preprocessing dataset...\")\n",
        "try:\n",
        "    dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names, desc=\"Processing\")\n",
        "    print(f\"‚úÖ Preprocessed {len(dataset)} samples\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Preprocessing failed: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ============================================================\n",
        "# Step 8: Training Setup\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 7: Training Configuration\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=CONFIG[\"OUTPUT_DIR\"],\n",
        "        per_device_train_batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "        gradient_accumulation_steps=CONFIG[\"GRAD_ACCUM\"],\n",
        "        num_train_epochs=CONFIG[\"EPOCHS\"],\n",
        "        learning_rate=CONFIG[\"LEARNING_RATE\"],\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        logging_steps=5,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=CONFIG[\"WARMUP_RATIO\"],\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    print(f\"Effective batch size: {CONFIG['BATCH_SIZE'] * CONFIG['GRAD_ACCUM']}\")\n",
        "    print(f\"Total epochs: {CONFIG['EPOCHS']}\")\n",
        "    print(f\"Learning rate: {CONFIG['LEARNING_RATE']}\")\n",
        "    print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    print(\"‚úÖ Trainer initialized\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training setup failed: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ============================================================\n",
        "# Step 9: Train!\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 8: STARTING TRAINING\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n‚úÖ Training finished!\\n\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ============================================================\n",
        "# Step 10: Save Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 9: Saving Model & Adapter\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    print(f\"üíæ Saving to {CONFIG['OUTPUT_DIR']}...\")\n",
        "    model.save_pretrained(CONFIG[\"OUTPUT_DIR\"])\n",
        "    tokenizer.save_pretrained(CONFIG[\"OUTPUT_DIR\"])\n",
        "\n",
        "    print(f\"\\n‚úÖ ALL DONE!\")\n",
        "    print(f\"\\nüìÅ LoRA adapter saved to: {CONFIG['OUTPUT_DIR']}\")\n",
        "    print(\"\\nüìÇ Files created:\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ adapter_config.json\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ adapter_model.bin\")\n",
        "    print(\"   ‚îî‚îÄ‚îÄ tokenizer files\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to save model: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6itEJfBALoW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}